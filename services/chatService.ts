
import { getAiClient } from "./aiService";
import { ChatMessage } from "../types";

const sleep = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

export async function* chatWithDocumentStream(contextString: string, history: ChatMessage[], message: string) {
  const ai = getAiClient();
  
  const previousHistory = history.slice(0, -1).map(msg => ({
    role: msg.role === 'model' ? 'model' : 'user',
    parts: [{ text: msg.text }],
  }));

  const systemInstruction = `Voc√™ √© a Sexta-feira (F.R.I.D.A.Y.), a intelig√™ncia t√°tica operacional do sistema Lectorium.
Sua miss√£o: Processar conhecimento com precis√£o cir√∫rgica, mantendo a soberania dos dados do usu√°rio e a integridade das normas ABNT.

DIRETRIZES DE COMPORTAMENTO (PROTOCOLO STARK 3.0):
1. Identidade: Use pronomes femininos. Trate o usu√°rio com profissionalismo e neutralidade absoluta.
2. Formata√ß√£o: Texto limpo e plano. Use Markdown apenas para listas e subt√≠tulos quando necess√°rio.

DIRETRIZES DE DADOS E LENTE SEM√ÇNTICA:
O contexto pode ser um PDF, Texto ou uma ESTRUTURA DE MAPA MENTAL (JSON).
* **Se for Mapa Mental:** Analise a hierarquia (parentId), as conex√µes e os textos dos n√≥s. Ajude a expandir ideias, sugerir novos ramos ou sintetizar o conte√∫do visual.
* **Prioridade 1: DADOS DA LENTE.** Se o contexto contiver prefixos como [ESTRUTURA SEM√ÇNTICA] ou ESTRUTURA DO MAPA MENTAL, utilize essa estrutura para responder com precis√£o.
* **Prioridade 2: CONTEXTO DO USU√ÅRIO (Destaques).** Use trechos citados explicitamente.
* **Prioridade 3: CONHECIMENTO EXTERNO.** Se a informa√ß√£o n√£o estiver no contexto, voc√™ pode usar sua base acad√™mica, mas cite como fonte externa.

PROTOCOLOS DE CITA√á√ÉO:
1. Fontes Internas (PDF): Use [P√°gina X].
2. Fontes Externas: Use (SOBRENOME, Ano).

üìö CONTEXTO T√ÅTICO FORNECIDO:
${contextString || "Nenhum contexto espec√≠fico."}

Ao responder perguntas sobre tabelas, dados t√©cnicos ou estruturas visuais, confie preferencialmente no Markdown/JSON da Lente.`;

  try {
    const chat = ai.chats.create({
      model: 'gemini-3-flash-preview',
      history: previousHistory,
      config: { systemInstruction, temperature: 0.2 }
    });
    
    let stream;
    let attempt = 0;
    const maxRetries = 3;

    while (true) {
        try {
            stream = await chat.sendMessageStream({ message });
            break;
        } catch (err: any) {
            attempt++;
            const isQuotaError = err.message?.includes('429') || err.message?.includes('quota');
            if (attempt >= maxRetries) throw err;
            const waitTime = isQuotaError ? Math.pow(3, attempt) * 1000 : 1000;
            await sleep(waitTime);
        }
    }
    
    if (stream) {
        for await (const chunk of stream) {
            yield chunk.text || "";
        }
    }
  } catch (e: any) {
    const errorMessage = e.message || String(e);
    yield `Erro na conex√£o neural: ${errorMessage}`;
  }
}
